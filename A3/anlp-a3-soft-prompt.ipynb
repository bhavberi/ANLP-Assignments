{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:15.932007Z","iopub.status.busy":"2024-10-29T13:35:15.931169Z","iopub.status.idle":"2024-10-29T13:35:15.939955Z","shell.execute_reply":"2024-10-29T13:35:15.938974Z","shell.execute_reply.started":"2024-10-29T13:35:15.931966Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","import csv\n","import time\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from itertools import islice\n","from torchinfo import summary as modelinfo\n","# import matplotlib.pyplot as plt\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n","\n","\n","try: \n","    from rouge_score import rouge_scorer\n","except:\n","    %pip install rouge-score==0.1.2\n","    from rouge_score import rouge_scorer"]},{"cell_type":"markdown","metadata":{},"source":["## Config"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.455996Z","iopub.status.busy":"2024-10-29T13:35:16.455300Z","iopub.status.idle":"2024-10-29T13:35:16.460827Z","shell.execute_reply":"2024-10-29T13:35:16.459900Z","shell.execute_reply.started":"2024-10-29T13:35:16.455957Z"},"trusted":true},"outputs":[],"source":["train_filepath = \"/kaggle/input/cnn_dailymail/train.csv\"\n","val_filepath = \"/kaggle/input/cnn_dailymail/validation.csv\"\n","test_filepath = \"/kaggle/input/cnn_dailymail/test.csv\"\n","\n","# train_filepath = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\"\n","# val_filepath = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\"\n","# test_filepath = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\"\n","\n","max_train_samples = 5000\n","max_val_samples = 1000\n","max_test_samples = 1000"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.478512Z","iopub.status.busy":"2024-10-29T13:35:16.478193Z","iopub.status.idle":"2024-10-29T13:35:16.482928Z","shell.execute_reply":"2024-10-29T13:35:16.481985Z","shell.execute_reply.started":"2024-10-29T13:35:16.478480Z"},"trusted":true},"outputs":[],"source":["to_train = True\n","tuning_type=\"soft_prompt\"\n","model_name = \"EleutherAI/gpt-neo-125m\"\n","\n","lr = 6e-5\n","epochs = 5\n","mini_batch_size = 32\n","\n","PROMPT_TOKEN = \"[SUMMARIZE]\""]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.510898Z","iopub.status.busy":"2024-10-29T13:35:16.510499Z","iopub.status.idle":"2024-10-29T13:35:16.516672Z","shell.execute_reply":"2024-10-29T13:35:16.515496Z","shell.execute_reply.started":"2024-10-29T13:35:16.510856Z"},"trusted":true},"outputs":[],"source":["os.makedirs(\"models\", exist_ok=True)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.542214Z","iopub.status.busy":"2024-10-29T13:35:16.541897Z","iopub.status.idle":"2024-10-29T13:35:16.550479Z","shell.execute_reply":"2024-10-29T13:35:16.549192Z","shell.execute_reply.started":"2024-10-29T13:35:16.542182Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Random Seed: 42\n"]}],"source":["random_seed = 42\n","\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","print(\"Using Random Seed:\", random_seed)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.570658Z","iopub.status.busy":"2024-10-29T13:35:16.570324Z","iopub.status.idle":"2024-10-29T13:35:16.576221Z","shell.execute_reply":"2024-10-29T13:35:16.575183Z","shell.execute_reply.started":"2024-10-29T13:35:16.570626Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Utils"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.642628Z","iopub.status.busy":"2024-10-29T13:35:16.641670Z","iopub.status.idle":"2024-10-29T13:35:16.648762Z","shell.execute_reply":"2024-10-29T13:35:16.647773Z","shell.execute_reply.started":"2024-10-29T13:35:16.642578Z"},"trusted":true},"outputs":[],"source":["dm_single_close_quote = \"\\u2019\"  # unicode\n","dm_double_close_quote = \"\\u201d\"\n","\n","# acceptable ways to end a sentence\n","END_TOKENS = [\n","    \".\",\n","    \"!\",\n","    \"?\",\n","    \"'\",\n","    \"`\",\n","    '\"',\n","    dm_single_close_quote,\n","    dm_double_close_quote,\n","    \")\",\n","]"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:16.663569Z","iopub.status.busy":"2024-10-29T13:35:16.663256Z","iopub.status.idle":"2024-10-29T13:35:16.671191Z","shell.execute_reply":"2024-10-29T13:35:16.670140Z","shell.execute_reply.started":"2024-10-29T13:35:16.663537Z"},"trusted":true},"outputs":[],"source":["def remove_period(line):\n","    if line[-1] in END_TOKENS:\n","        return line[:-1]\n","    return line\n","\n","\n","def remove_punctuations(line):\n","    return re.sub(r\"[^\\w\\s]\", \" \", line)\n","\n","\n","def clean_data(data):\n","    for i in range(len(data)):\n","        data[i][\"article\"] = remove_punctuations(data[i][\"article\"])\n","        data[i][\"highlights\"] = remove_punctuations(data[i][\"highlights\"])\n","\n","        data[i][\"article\"] = re.sub(r\"\\s+\", \" \", data[i][\"article\"]).strip()\n","        data[i][\"highlights\"] = re.sub(r\"\\s+\", \" \", data[i][\"highlights\"]).strip()\n","\n","    return data"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:17.057011Z","iopub.status.busy":"2024-10-29T13:35:17.056354Z","iopub.status.idle":"2024-10-29T13:35:17.062682Z","shell.execute_reply":"2024-10-29T13:35:17.061645Z","shell.execute_reply.started":"2024-10-29T13:35:17.056969Z"},"trusted":true},"outputs":[],"source":["def read_data(filepath, max_length=None):\n","    with open(filepath, \"r\") as f:\n","        reader = csv.DictReader(f)\n","        if max_length is not None:\n","            rows = islice(reader, max_length)\n","        else:\n","            rows = reader\n","        data = list(rows)\n","    \n","    return clean_data(data)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:17.559448Z","iopub.status.busy":"2024-10-29T13:35:17.559064Z","iopub.status.idle":"2024-10-29T13:35:17.564661Z","shell.execute_reply":"2024-10-29T13:35:17.563500Z","shell.execute_reply.started":"2024-10-29T13:35:17.559410Z"},"trusted":true},"outputs":[],"source":["def make_tokenizer(model_name):\n","    \"\"\"Returns GPT2 tokenizer after adding separator and padding tokens\"\"\"\n","    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","    special_tokens = {\"pad_token\": \"<pad>\", \"sep_token\": \"<sep>\"}\n","    tokenizer.add_special_tokens(special_tokens)\n","    return tokenizer"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:54:50.887152Z","iopub.status.busy":"2024-10-29T14:54:50.886772Z","iopub.status.idle":"2024-10-29T14:54:50.901712Z","shell.execute_reply":"2024-10-29T14:54:50.900644Z","shell.execute_reply.started":"2024-10-29T14:54:50.887117Z"},"trusted":true},"outputs":[],"source":["class GPT2WithPromptTuning(torch.nn.Module):\n","    def __init__(self, model_name, num_prompts, tokenizer_len, embedding_size=768):\n","        super(GPT2WithPromptTuning, self).__init__()\n","        self.model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)\n","\n","        # Freeze all layers\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        self.model.resize_token_embeddings(tokenizer_len)\n","        self.soft_prompt_embeddings = torch.nn.Embedding(\n","            num_prompts, embedding_size\n","        ).to(device)\n","        # self.soft_prompt_embeddings.weight.data.copy_(\n","        #     self.model.transformer.wte\n","        # )\n","\n","        self.soft_prompt_embeddings.weight.data.copy_(\n","            self.model.transformer.wte.weight[:num_prompts]\n","        )\n","\n","    def forward(self, input_ids, soft_prompt_ids, attention_mask=None):\n","        prompt_embeddings = self.soft_prompt_embeddings(soft_prompt_ids.to(device)).to(device)\n","        base_embeddings = self.model.transformer.wte(input_ids).to(device)\n","        prompt_embeddings_expanded = (\n","            prompt_embeddings.unsqueeze(0)\n","            .expand(\n","                base_embeddings.shape[0],\n","                prompt_embeddings.shape[0],\n","                prompt_embeddings.shape[1],\n","            )\n","            .to(device)\n","        )\n","        embeddings = torch.cat([prompt_embeddings_expanded, base_embeddings], dim=1).to(device)\n","        if attention_mask is not None:\n","            attention_mask = torch.cat(\n","                [\n","                    torch.ones(attention_mask.shape[0], prompt_embeddings.shape[0]).to(\n","                        device\n","                    ),\n","                    attention_mask,\n","                ],\n","                dim=1,\n","            )\n","            outputs = self.model(\n","                inputs_embeds=embeddings, attention_mask=attention_mask\n","            )\n","        else:\n","            outputs = self.model(inputs_embeds=embeddings)\n","        return outputs\n","\n","    def generate(self, input, max_new_tokens, tokenizer, soft_prompt_ids):\n","        prompt_embeddings = self.soft_prompt_embeddings(soft_prompt_ids.to(device)).to(device)\n","        base_embeddings = self.model.transformer.wte(input).to(device)\n","        prompt_embeddings_expanded = (\n","            prompt_embeddings.unsqueeze(0)\n","            .expand(\n","                base_embeddings.shape[0],\n","                prompt_embeddings.shape[0],\n","                prompt_embeddings.shape[1],\n","            )\n","            .to(device)\n","        )\n","        embeddings = torch.cat([prompt_embeddings_expanded, base_embeddings], dim=1)\n","        outputs = self.model.generate(\n","            inputs_embeds=embeddings,\n","            max_new_tokens=max_new_tokens,\n","            pad_token_id=tokenizer.pad_token_id,\n","            eos_token_id=tokenizer.eos_token_id,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.95,\n","            temperature=0.7,\n","            num_return_sequences=1,\n","            no_repeat_ngram_size=3,\n","            num_beams=4,\n","            early_stopping=True,\n","        )\n","\n","        return outputs"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:53:49.794698Z","iopub.status.busy":"2024-10-29T14:53:49.794046Z","iopub.status.idle":"2024-10-29T14:53:49.810769Z","shell.execute_reply":"2024-10-29T14:53:49.809797Z","shell.execute_reply.started":"2024-10-29T14:53:49.794659Z"},"trusted":true},"outputs":[],"source":["class SummarizationDataset(Dataset):\n","    def __init__(self, data, tokenizer, article_max_length=512, summary_max_length=128, type=\"train\"):\n","        assert type in [\"train\", \"val\", \"test\"], \"Invalid dataset type\"\n","\n","        self.type = type\n","        if type == \"test\":\n","            summary_max_length = 0\n","\n","        self.tokenizer = tokenizer\n","        self.article_max_length = article_max_length\n","        self.summary_max_length = summary_max_length\n","        self.instruction_tokens = self.tokenizer.encode(\"summarize: \")\n","        self.sep_token = self.tokenizer.encode(\" \" + self.tokenizer.sep_token + \" \")\n","        self.max_length = article_max_length + summary_max_length + len(self.instruction_tokens) + len(self.sep_token)\n","        \n","        self.processed_data = self._process_all_data(data, 4)\n","\n","    def _attention_mask(self, padding_length):\n","        return [1] * (self.max_length - padding_length) + [0] * padding_length\n","\n","    def _process_data(self, data):\n","        article_ids = self.tokenizer.encode(\n","            data[\"article\"], truncation=True, max_length=self.article_max_length\n","        )\n","        if self.type != \"test\":\n","            abstract_ids = self.tokenizer.encode(\n","                data[\"highlights\"], truncation=True, max_length=self.summary_max_length\n","            )\n","        else:\n","            abstract_ids = []\n","\n","        # Combine all components\n","        content = self.instruction_tokens + article_ids + self.sep_token + abstract_ids\n","\n","        if self.type != \"test\":\n","            padding_length = self.max_length - len(content)\n","            padded_content = content + [self.tokenizer.pad_token_id] * padding_length\n","        else:\n","            padding_length = self.max_length - len(content)\n","            padded_content = [self.tokenizer.pad_token_id] * padding_length + content\n","\n","        return {\n","            \"text\": padded_content,\n","            \"sep_idx\": len(article_ids),\n","            \"article_len\": len(article_ids),\n","            \"summary_len\": len(abstract_ids),\n","            \"attention_mask\": self._attention_mask(padding_length),\n","            \"highlights\": data[\"highlights\"],\n","        }\n","    \n","    def _process_all_data(self, data, num_workers):\n","        processed_data = []\n","\n","        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n","            # Submit tasks to the thread pool\n","            futures = [executor.submit(self._process_data, item) for item in data]\n","            \n","            # Use tqdm to track the progress\n","            for future in tqdm(as_completed(futures), total=len(data), desc=\"Processing Data\"):\n","                processed_data.append(future.result())\n","\n","        return processed_data\n","\n","    def __len__(self):\n","        return len(self.processed_data)\n","\n","    def __getitem__(self, idx):\n","        processed_item = self.processed_data[idx]\n","        return {\n","            \"article\": torch.tensor(processed_item[\"text\"]),\n","            \"sep_idx\": processed_item[\"sep_idx\"],\n","            \"article_len\": processed_item[\"article_len\"],\n","            \"summary_len\": processed_item[\"summary_len\"],\n","            \"attention_mask\": torch.tensor(processed_item[\"attention_mask\"]),\n","            \"highlights\": processed_item[\"highlights\"],\n","        }"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:53:50.164515Z","iopub.status.busy":"2024-10-29T14:53:50.163844Z","iopub.status.idle":"2024-10-29T14:53:50.173804Z","shell.execute_reply":"2024-10-29T14:53:50.172751Z","shell.execute_reply.started":"2024-10-29T14:53:50.164478Z"},"trusted":true},"outputs":[],"source":["def evaluate(model, val_loader, loss_fn, prompt_id, summary_max_length=128):\n","    \"\"\"\n","    Evaluate the model on validation/test data\n","\n","    Args:\n","        model: The model to evaluate\n","        val_loader: DataLoader for validation data\n","        loss_fn: Loss function (typically CrossEntropyLoss with ignore_index set to pad_token_id)\n","\n","    Returns:\n","        float: Average loss per batch\n","    \"\"\"\n","    model.eval()\n","    total_loss = 0.0\n","    num_batches = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n","            inputs = batch[\"article\"].to(device)\n","            sep_idx = batch[\"sep_idx\"].squeeze() + 1\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            outputs = model(inputs, attention_mask=attention_mask, soft_prompt_ids=prompt_id)\n","            # loss = outputs.loss\n","            # decoded_output = tokenizer.decode(outputs.logits[0].argmax(dim=-1))\n","            # decoded_input = tokenizer.decode(inputs[0])\n","            # print(decoded_input, decoded_output, sep=\"\\n\\n\")\n","\n","            # Get logits and labels for summary portion only\n","            shift_logits = outputs.logits[..., sep_idx:-1, :].contiguous()\n","            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n","            shift_labels = inputs[..., sep_idx + 1 :].contiguous()\n","            shift_labels = shift_labels.view(-1)\n","\n","            shift_logits = shift_logits[:summary_max_length]\n","            shift_labels = shift_labels[:summary_max_length]\n","            \n","            # print()\n","            # print(tokenizer.decode(shift_logits.argmax(dim=-1)))\n","            # print(shift_labels)\n","\n","            loss = loss_fn(shift_logits, shift_labels)\n","            total_loss += loss.item()\n","            num_batches += 1\n","\n","    return total_loss / (num_batches if num_batches > 0 else 1)"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:53:50.887795Z","iopub.status.busy":"2024-10-29T14:53:50.886946Z","iopub.status.idle":"2024-10-29T14:53:50.900660Z","shell.execute_reply":"2024-10-29T14:53:50.899779Z","shell.execute_reply.started":"2024-10-29T14:53:50.887753Z"},"trusted":true},"outputs":[],"source":["def train(\n","    model,\n","    optimiser,\n","    loss_fn,\n","    train_loader,\n","    val_loader,\n","    num_epochs,\n","    prompt_id,\n","    max_grad_norm=1.0,\n","    mini_batch_size=4,\n","    summary_max_length=128,\n","    save_path=\"models/model.pt\",\n","    print_every=5,\n","):\n","    global_step = 0\n","    best_val_loss = float(\"inf\")\n","    tr_loss = 0.0\n","    last_tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","\n","        for step, batch in enumerate(epoch_iterator):\n","            inputs = batch[\"article\"].to(device)\n","            sep_idx = batch[\"sep_idx\"].squeeze() + 1\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            outputs = model(inputs, soft_prompt_ids=prompt_id, attention_mask=attention_mask)\n","\n","            shift_logits = outputs.logits[..., sep_idx:-1, :].contiguous()\n","            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n","            shift_labels = inputs[..., sep_idx + 1 :].contiguous()\n","            shift_labels = shift_labels.view(-1)\n","\n","            shift_logits = shift_logits[:summary_max_length]\n","            shift_labels = shift_labels[:summary_max_length]\n","\n","            loss = loss_fn(shift_logits, shift_labels)\n","            loss /= mini_batch_size\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","            tr_loss += loss.item()\n","\n","            if (step + 1) % mini_batch_size == 0:\n","                optimiser.step()\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if global_step % print_every == 0:\n","                    log_loss = (tr_loss - logging_loss) / mini_batch_size\n","                    logging_loss = tr_loss\n","\n","                    print(f\"Step: {global_step}, Mini-Batch Loss: {log_loss:.4f}\")\n","\n","        print(\n","            f\"Epoch: {epoch+1}, Avg Training Loss: {(tr_loss - last_tr_loss) / len(train_loader):.4f}/sample\",\n","            end=\", \",\n","        )\n","        last_tr_loss = tr_loss\n","\n","        val_loss = evaluate(model, val_loader, loss_fn, prompt_id)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), save_path)\n","\n","        print(f\"Validation Loss: {val_loss:.4f}\")"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:53:51.211882Z","iopub.status.busy":"2024-10-29T14:53:51.211136Z","iopub.status.idle":"2024-10-29T14:53:51.220894Z","shell.execute_reply":"2024-10-29T14:53:51.219991Z","shell.execute_reply.started":"2024-10-29T14:53:51.211838Z"},"trusted":true},"outputs":[],"source":["def predict(\n","    model,\n","    tokenizer,\n","    text,\n","    article_max_length=512,\n","    max_length=128,\n","    preprocess=True,\n","    sep_idx=None,\n","    prompt_id=0,\n","):\n","    model.eval()\n","    if preprocess:\n","        text_ids = (\n","            tokenizer.encode(\"summarize: \")\n","            + tokenizer.encode(text)[:article_max_length]\n","            + tokenizer.encode(tokenizer.sep_token)\n","        )\n","        sep_idx = len(text_ids) - 1\n","        inputs = torch.tensor(text_ids).unsqueeze(0).to(device)\n","    else:\n","        assert sep_idx is not None, \"sep_idx must be provided if preprocess is False\"\n","        assert isinstance(sep_idx, torch.Tensor), \"sep_idx must be a list\"\n","        inputs = text.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            inputs,\n","            max_new_tokens=max_length,\n","            tokenizer=tokenizer,\n","            soft_prompt_ids=prompt_id,\n","        )\n","\n","    if preprocess:\n","        generated_summary = tokenizer.decode(\n","            outputs[0][sep_idx + 1 :], skip_special_tokens=True\n","        )\n","        return generated_summary\n","    else:\n","        generated_summaries = []\n","        for i in range(len(sep_idx)):\n","            sep_idx_i = sep_idx[i].item() + 1\n","            generated_summary = tokenizer.decode(outputs[i][sep_idx_i + 1 :], skip_special_tokens=True)\n","            generated_summaries.append(generated_summary)\n","\n","        return generated_summaries"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:53:51.787547Z","iopub.status.busy":"2024-10-29T14:53:51.786869Z","iopub.status.idle":"2024-10-29T14:53:51.795444Z","shell.execute_reply":"2024-10-29T14:53:51.794457Z","shell.execute_reply.started":"2024-10-29T14:53:51.787507Z"},"trusted":true},"outputs":[],"source":["def test_score(model, tokenizer, test_loader, article_max_length=512, max_length=128, print_every=False, prompt_id=0):\n","    rouge_scorer_obj = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"])\n","    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n","\n","    for i in tqdm(test_loader):\n","        text = i[\"article\"]\n","        summary = i[\"highlights\"]\n","\n","        generated_summary = predict(\n","            model,\n","            tokenizer,\n","            text,\n","            article_max_length,\n","            max_length,\n","            preprocess=False,\n","            sep_idx=i[\"sep_idx\"],\n","            prompt_id=prompt_id,\n","        )\n","\n","        for j in range(len(summary)):\n","            scores = rouge_scorer_obj.score(summary[j], generated_summary[j])\n","            for key in rouge_scores:\n","                rouge_scores[key].append(scores[key].fmeasure)\n","            \n","            if print_every:\n","                print(scores)\n","\n","    for key in rouge_scores:\n","        rouge_scores[key] = np.mean(rouge_scores[key])\n","\n","    return rouge_scores"]},{"cell_type":"markdown","metadata":{},"source":["## Main"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:17.830786Z","iopub.status.busy":"2024-10-29T13:35:17.830466Z","iopub.status.idle":"2024-10-29T13:35:21.562030Z","shell.execute_reply":"2024-10-29T13:35:21.561173Z","shell.execute_reply.started":"2024-10-29T13:35:17.830753Z"},"trusted":true},"outputs":[],"source":["train_data = read_data(train_filepath, max_train_samples)\n","val_data = read_data(val_filepath, max_val_samples)\n","test_data = read_data(test_filepath, max_test_samples)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:35:21.564174Z","iopub.status.busy":"2024-10-29T13:35:21.563845Z","iopub.status.idle":"2024-10-29T13:35:21.924007Z","shell.execute_reply":"2024-10-29T13:35:21.923180Z","shell.execute_reply.started":"2024-10-29T13:35:21.564141Z"},"trusted":true},"outputs":[],"source":["tokenizer = make_tokenizer(model_name)\n","len_tokenizer = len(tokenizer)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:36:03.874082Z","iopub.status.busy":"2024-10-29T13:36:03.873228Z","iopub.status.idle":"2024-10-29T13:36:03.880522Z","shell.execute_reply":"2024-10-29T13:36:03.879561Z","shell.execute_reply.started":"2024-10-29T13:36:03.874044Z"},"trusted":true},"outputs":[],"source":["soft_prompt_vocab = [PROMPT_TOKEN]\n","soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n","num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n","prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n","prompt_encoded = torch.tensor(tokenizer.encode(PROMPT_TOKEN)).to(device)"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:55:00.041589Z","iopub.status.busy":"2024-10-29T14:55:00.041227Z","iopub.status.idle":"2024-10-29T14:55:01.146563Z","shell.execute_reply":"2024-10-29T14:55:01.145706Z","shell.execute_reply.started":"2024-10-29T14:55:00.041557Z"},"trusted":true},"outputs":[],"source":["model = GPT2WithPromptTuning(\n","    model_name, \n","    num_prompts, \n","    tokenizer_len=len_tokenizer,\n","#     prompt_encoded=prompt_encoded\n",").to(device)"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:55:01.148493Z","iopub.status.busy":"2024-10-29T14:55:01.148161Z","iopub.status.idle":"2024-10-29T14:55:01.184324Z","shell.execute_reply":"2024-10-29T14:55:01.183383Z","shell.execute_reply.started":"2024-10-29T14:55:01.148458Z"},"trusted":true},"outputs":[{"data":{"text/plain":["=====================================================================================\n","Layer (type:depth-idx)                                       Param #\n","=====================================================================================\n","GPT2WithPromptTuning                                         --\n","├─GPTNeoForCausalLM: 1-1                                     --\n","│    └─GPTNeoModel: 2-1                                      --\n","│    │    └─Embedding: 3-1                                   (38,598,912)\n","│    │    └─Embedding: 3-2                                   (1,572,864)\n","│    │    └─Dropout: 3-3                                     --\n","│    │    └─ModuleList: 3-4                                  (85,026,816)\n","│    │    └─LayerNorm: 3-5                                   (1,536)\n","│    └─Linear: 2-2                                           (38,598,912)\n","├─Embedding: 1-2                                             768\n","=====================================================================================\n","Total params: 163,799,808\n","Trainable params: 768\n","Non-trainable params: 163,799,040\n","====================================================================================="]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["modelinfo(model)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:45:36.347873Z","iopub.status.busy":"2024-10-29T13:45:36.347226Z","iopub.status.idle":"2024-10-29T13:47:43.509597Z","shell.execute_reply":"2024-10-29T13:47:43.508594Z","shell.execute_reply.started":"2024-10-29T13:45:36.347833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing Data: 100%|██████████| 5000/5000 [01:32<00:00, 53.96it/s]\n","Processing Data: 100%|██████████| 1000/1000 [00:18<00:00, 55.11it/s]\n","Processing Data: 100%|██████████| 1000/1000 [00:16<00:00, 62.16it/s]\n"]}],"source":["if to_train:\n","    train_dataset = SummarizationDataset(train_data, tokenizer, type=\"train\")\n","    val_dataset = SummarizationDataset(val_data, tokenizer, type=\"val\")\n","test_dataset = SummarizationDataset(test_data, tokenizer, type=\"test\")\n","\n","if to_train:\n","    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:49:01.695534Z","iopub.status.busy":"2024-10-29T13:49:01.694773Z","iopub.status.idle":"2024-10-29T13:49:01.701421Z","shell.execute_reply":"2024-10-29T13:49:01.700325Z","shell.execute_reply.started":"2024-10-29T13:49:01.695497Z"},"trusted":true},"outputs":[],"source":["optimiser = optim.AdamW(model.parameters(), lr=lr)\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T13:49:02.763014Z","iopub.status.busy":"2024-10-29T13:49:02.762152Z","iopub.status.idle":"2024-10-29T14:30:21.144982Z","shell.execute_reply":"2024-10-29T14:30:21.143993Z","shell.execute_reply.started":"2024-10-29T13:49:02.762973Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1:  26%|██▌       | 1281/5000 [01:56<05:36, 11.06it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 40, Mini-Batch Loss: 24.7587\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:  51%|█████     | 2561/5000 [03:51<03:40, 11.08it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 80, Mini-Batch Loss: 24.6570\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:  77%|███████▋  | 3841/5000 [05:47<01:45, 11.01it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 120, Mini-Batch Loss: 24.7093\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 5000/5000 [07:31<00:00, 11.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Avg Training Loss: 0.6177/sample, "]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 1000/1000 [00:43<00:00, 23.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 19.9644\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:   3%|▎         | 130/5000 [00:11<07:19, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 160, Mini-Batch Loss: 24.8784\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:  28%|██▊       | 1410/5000 [02:07<05:24, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 200, Mini-Batch Loss: 24.8245\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:  54%|█████▍    | 2690/5000 [04:02<03:28, 11.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 240, Mini-Batch Loss: 24.6298\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:  79%|███████▉  | 3970/5000 [05:58<01:32, 11.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 280, Mini-Batch Loss: 24.6828\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 5000/5000 [07:31<00:00, 11.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2, Avg Training Loss: 0.6176/sample, "]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 1000/1000 [00:43<00:00, 23.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 19.9633\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3:   5%|▌         | 258/5000 [00:23<07:08, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 320, Mini-Batch Loss: 24.7531\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3:  31%|███       | 1538/5000 [02:18<05:12, 11.06it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 360, Mini-Batch Loss: 24.7734\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3:  56%|█████▋    | 2818/5000 [04:14<03:17, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 400, Mini-Batch Loss: 24.6374\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3:  82%|████████▏ | 4098/5000 [06:09<01:21, 11.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 440, Mini-Batch Loss: 24.8711\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 5000/5000 [07:31<00:00, 11.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3, Avg Training Loss: 0.6174/sample, "]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 1000/1000 [00:43<00:00, 23.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 19.9622\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4:   8%|▊         | 386/5000 [00:34<06:55, 11.11it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 480, Mini-Batch Loss: 24.7459\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4:  33%|███▎      | 1666/5000 [02:30<05:01, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 520, Mini-Batch Loss: 24.7071\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4:  59%|█████▉    | 2946/5000 [04:25<03:05, 11.05it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 560, Mini-Batch Loss: 24.6674\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4:  85%|████████▍ | 4226/5000 [06:21<01:10, 11.06it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 600, Mini-Batch Loss: 24.6094\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 5000/5000 [07:30<00:00, 11.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4, Avg Training Loss: 0.6172/sample, "]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 1000/1000 [00:43<00:00, 23.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 19.9742\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5:  10%|█         | 514/5000 [00:46<06:45, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 640, Mini-Batch Loss: 25.0547\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5:  36%|███▌      | 1794/5000 [02:42<04:49, 11.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 680, Mini-Batch Loss: 24.7180\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5:  61%|██████▏   | 3074/5000 [04:37<02:54, 11.05it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 720, Mini-Batch Loss: 24.6010\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5:  87%|████████▋ | 4354/5000 [06:33<00:58, 11.05it/s]"]},{"name":"stdout","output_type":"stream","text":["Step: 760, Mini-Batch Loss: 24.6908\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 5000/5000 [07:31<00:00, 11.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5, Avg Training Loss: 0.6169/sample, "]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 1000/1000 [00:43<00:00, 23.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 19.9612\n","Total Training Time: 41.30 minutes\n","\n","Model loaded successfully\n"]}],"source":["if to_train:\n","    start_time = time.time()\n","    train(\n","        model,\n","        optimiser,\n","        loss_fn,\n","        train_loader,\n","        val_loader,\n","        prompt_id=prompt_id,\n","        num_epochs=epochs,\n","        mini_batch_size=mini_batch_size,\n","        save_path=f\"models/model_{tuning_type}.pt\",\n","        print_every=40,\n","    )\n","    end_time = time.time()\n","    total_training_time = end_time - start_time\n","    print(f\"Total Training Time: {total_training_time / 60:.2f} minutes\\n\")\n","\n","if os.path.exists(f\"models/model_{tuning_type}.pt\"):\n","    model.load_state_dict(torch.load(f\"models/model_{tuning_type}.pt\", map_location=device, weights_only=True))\n","    print(\"Model loaded successfully\")\n","else:\n","    print(\"No model checkpoint found\")"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:49:11.837961Z","iopub.status.busy":"2024-10-29T14:49:11.837280Z","iopub.status.idle":"2024-10-29T14:49:11.842284Z","shell.execute_reply":"2024-10-29T14:49:11.841422Z","shell.execute_reply.started":"2024-10-29T14:49:11.837919Z"},"trusted":true},"outputs":[],"source":["# # Predict first 5 examples from test data\n","\n","# for i in range(5):\n","#     text = test_data[i][\"article\"]\n","#     summary = test_data[i][\"highlights\"]\n","#     generated_summary = predict(model, tokenizer, text)\n","\n","#     print(f\"Example {i+1}\")\n","#     print(\"Text:\", text)\n","#     print(\"Actual Summary:\", summary)\n","#     print(\"Generated Summary:\", generated_summary)\n","#     print(\"\\n\")"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T14:55:07.168032Z","iopub.status.busy":"2024-10-29T14:55:07.167183Z","iopub.status.idle":"2024-10-29T15:08:30.862714Z","shell.execute_reply":"2024-10-29T15:08:30.861782Z","shell.execute_reply.started":"2024-10-29T14:55:07.167991Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [13:23<00:00, 12.76s/it]"]},{"name":"stdout","output_type":"stream","text":["Test Scores: {'rouge1': 0.11093297419067121, 'rouge2': 0.004512371058043215, 'rougeL': 0.109667575147095618}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["print(\"Test Scores:\", test_score(model, tokenizer, test_loader, print_every=False, prompt_id=prompt_id))"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T15:08:36.590570Z","iopub.status.busy":"2024-10-29T15:08:36.589681Z","iopub.status.idle":"2024-10-29T15:08:36.597040Z","shell.execute_reply":"2024-10-29T15:08:36.596166Z","shell.execute_reply.started":"2024-10-29T15:08:36.590529Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Allocated Memory: 2.0837 GB\n","Max Allocated Memory: 10.9778 GB\n","Reserved Memory: 13.6719 GB\n","Max Reserved Memory: 13.6719 GB\n"]}],"source":["print(f\"Allocated Memory: {torch.cuda.memory_allocated() / (1024 ** 3):.4f} GB\")\n","print(f\"Max Allocated Memory: {torch.cuda.max_memory_allocated() / (1024 ** 3):.4f} GB\")\n","print(f\"Reserved Memory: {torch.cuda.memory_reserved() / (1024 ** 3):.4f} GB\")\n","print(f\"Max Reserved Memory: {torch.cuda.max_memory_reserved() / (1024 ** 3):.4f} GB\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1654566,"sourceId":2734496,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
