{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:36:50.506569Z",
     "iopub.status.busy": "2024-11-08T08:36:50.505746Z",
     "iopub.status.idle": "2024-11-08T08:37:15.099076Z",
     "shell.execute_reply": "2024-11-08T08:37:15.097996Z",
     "shell.execute_reply.started": "2024-11-08T08:36:50.506525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.46.2\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.2) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (2024.8.30)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "Successfully installed transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers==4.46.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:15.101284Z",
     "iopub.status.busy": "2024-11-08T08:37:15.100977Z",
     "iopub.status.idle": "2024-11-08T08:37:19.340013Z",
     "shell.execute_reply": "2024-11-08T08:37:19.339226Z",
     "shell.execute_reply.started": "2024-11-08T08:37:15.101250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import profiler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.341502Z",
     "iopub.status.busy": "2024-11-08T08:37:19.341099Z",
     "iopub.status.idle": "2024-11-08T08:37:19.345858Z",
     "shell.execute_reply": "2024-11-08T08:37:19.344818Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.341469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset_path = \"/kaggle/input/penn-treebank-dataset/ptbdataset/ptb.test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.348824Z",
     "iopub.status.busy": "2024-11-08T08:37:19.348463Z",
     "iopub.status.idle": "2024-11-08T08:37:19.546660Z",
     "shell.execute_reply": "2024-11-08T08:37:19.545610Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.348781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "# model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "model_name = \"allenai/OLMo-1B-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.548146Z",
     "iopub.status.busy": "2024-11-08T08:37:19.547800Z",
     "iopub.status.idle": "2024-11-08T08:37:19.556547Z",
     "shell.execute_reply": "2024-11-08T08:37:19.555732Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.548112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "partial_no_layers_quantise = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.557867Z",
     "iopub.status.busy": "2024-11-08T08:37:19.557588Z",
     "iopub.status.idle": "2024-11-08T08:37:19.592465Z",
     "shell.execute_reply": "2024-11-08T08:37:19.591641Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.557836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.593688Z",
     "iopub.status.busy": "2024-11-08T08:37:19.593409Z",
     "iopub.status.idle": "2024-11-08T08:37:19.605817Z",
     "shell.execute_reply": "2024-11-08T08:37:19.604955Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.593658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.607646Z",
     "iopub.status.busy": "2024-11-08T08:37:19.606988Z",
     "iopub.status.idle": "2024-11-08T08:37:19.616731Z",
     "shell.execute_reply": "2024-11-08T08:37:19.615948Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.607602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_data(filepath, limit=None):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    data = [line.strip().replace(\"\\n\", \"<eos>\") for line in data]\n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "    data = \"\\n\".join(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.618152Z",
     "iopub.status.busy": "2024-11-08T08:37:19.617839Z",
     "iopub.status.idle": "2024-11-08T08:37:19.628025Z",
     "shell.execute_reply": "2024-11-08T08:37:19.627318Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.618114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, encodings):\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = max_length // 2  # To avoid too much truncation\n",
    "    nlls = []\n",
    "\n",
    "    encodings = encodings.to(device)\n",
    "    \n",
    "    with profiler.profile(use_device=str(device), use_cpu=False, use_kineto=True) as prof:\n",
    "        for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "            begin_loc = i\n",
    "            end_loc = min(i + max_length, encodings.input_ids.size(1))\n",
    "            trg_len = end_loc - begin_loc  # Target length\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "                neg_log_likelihood = outputs.loss * trg_len\n",
    "            \n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "    perplexity = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    profiler_obj = prof.total_average()\n",
    "    \n",
    "    return perplexity.item(), profiler_obj, len(range(0, encodings.input_ids.size(1), stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.632293Z",
     "iopub.status.busy": "2024-11-08T08:37:19.631996Z",
     "iopub.status.idle": "2024-11-08T08:37:19.643074Z",
     "shell.execute_reply": "2024-11-08T08:37:19.642277Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.632250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class INT8Layer(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True, dtype=torch.float32):\n",
    "        super(INT8Layer, self).__init__()\n",
    "\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randint(\n",
    "                -128, 127, (output_features, input_features), dtype=torch.int8\n",
    "            ),\n",
    "        )\n",
    "        self.register_buffer(\"scales\", torch.randn((output_features), dtype=dtype))\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, output_features), dtype=dtype))\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        quantized_weights = self.weight.to(input.dtype)\n",
    "        output = F.linear(input, quantized_weights) * self.scales\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "    def quantize(self, weights, bias):\n",
    "        if bias is not None:\n",
    "            self.bias = bias.clone()\n",
    "\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "        self.weight = (weights / scales.unsqueeze(-1)).to(torch.int8)\n",
    "        self.scales = scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.644485Z",
     "iopub.status.busy": "2024-11-08T08:37:19.644167Z",
     "iopub.status.idle": "2024-11-08T08:37:19.659212Z",
     "shell.execute_reply": "2024-11-08T08:37:19.658433Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.644454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def replace_linear_layer(model, layer):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     for name, child in model.named_children():\n",
    "#         if isinstance(child, nn.Linear):\n",
    "#             og_bias = child.bias\n",
    "#             og_weights = child.weight\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 new_layer = layer(\n",
    "#                     child.in_features,\n",
    "#                     child.out_features,\n",
    "#                     bias=og_bias is not None,\n",
    "#                     dtype=og_weights.dtype,\n",
    "#                 )\n",
    "\n",
    "#             setattr(model, name, new_layer)\n",
    "#             getattr(model, name).quantize(og_weights, og_bias)\n",
    "#         else:\n",
    "#             print(\"Calling\", name)\n",
    "#             replace_linear_layer(child, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.660572Z",
     "iopub.status.busy": "2024-11-08T08:37:19.660276Z",
     "iopub.status.idle": "2024-11-08T08:37:19.674115Z",
     "shell.execute_reply": "2024-11-08T08:37:19.673314Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.660542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def replace_linear_layer(model, layer, partial=False, cont=False, quantise_n=8):\n",
    "    torch.cuda.empty_cache()\n",
    "    for name, child in model.named_children():\n",
    "        cont1 = True\n",
    "        if partial and not cont:\n",
    "            cont1 = name.isdigit() and int(name) < quantise_n\n",
    "        if cont1 and isinstance(child, nn.Linear):\n",
    "            og_bias = child.bias\n",
    "            og_weights = child.weight\n",
    "\n",
    "            with torch.no_grad():\n",
    "                new_layer = layer(\n",
    "                    child.in_features,\n",
    "                    child.out_features,\n",
    "                    bias=og_bias is not None,\n",
    "                    dtype=og_weights.dtype,\n",
    "                )\n",
    "\n",
    "            setattr(model, name, new_layer)\n",
    "            getattr(model, name).quantize(og_weights, og_bias)\n",
    "        else:\n",
    "            replace_linear_layer(child, layer, partial=partial, cont=cont1, quantise_n=quantise_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.675521Z",
     "iopub.status.busy": "2024-11-08T08:37:19.675189Z",
     "iopub.status.idle": "2024-11-08T08:37:19.684827Z",
     "shell.execute_reply": "2024-11-08T08:37:19.684004Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.675480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def testing(model, test_encodings, desc=\"\"):\n",
    "    # Run the perplexity calculation and profiling\n",
    "    perplexity, profiler_obj, n_items = calculate_perplexity(model, test_encodings)\n",
    "    memory_footprint_before_quantization = model.get_memory_footprint() / 1e6\n",
    "\n",
    "    cuda_time_ms = profiler_obj.device_time / 1e3\n",
    "    inference_latency_ms = cuda_time_ms / n_items\n",
    "\n",
    "    print(desc)\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "    print(f\"Cuda Time: {cuda_time_ms:.4f} ms\")\n",
    "    print(f\"Inference Latency: {inference_latency_ms:.4f} ms per inference\")\n",
    "    print(f\"Memory Footprint: {memory_footprint_before_quantization:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:37:19.686260Z",
     "iopub.status.busy": "2024-11-08T08:37:19.685952Z",
     "iopub.status.idle": "2024-11-08T08:39:28.111445Z",
     "shell.execute_reply": "2024-11-08T08:39:28.110134Z",
     "shell.execute_reply.started": "2024-11-08T08:37:19.686224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2df86021de494e9788ae2eb479c162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c760e4d62b44e1ab612e884a0a63d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ecc1988cd84e13a20c720c351baf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b32740307e3472c9f6e45ae5865dfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0eb653b77b4fc2a43329291e913ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ba79bec3714351bbb1ffc5ab355cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:39:28.115783Z",
     "iopub.status.busy": "2024-11-08T08:39:28.114554Z",
     "iopub.status.idle": "2024-11-08T08:39:28.609741Z",
     "shell.execute_reply": "2024-11-08T08:39:28.608681Z",
     "shell.execute_reply.started": "2024-11-08T08:39:28.115734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data = read_data(test_dataset_path)\n",
    "test_encodings = tokenizer(test_data, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:39:28.614478Z",
     "iopub.status.busy": "2024-11-08T08:39:28.614036Z",
     "iopub.status.idle": "2024-11-08T08:40:50.410747Z",
     "shell.execute_reply": "2024-11-08T08:40:50.409792Z",
     "shell.execute_reply.started": "2024-11-08T08:39:28.614418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:09<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Quantization:\n",
      "Perplexity: 218.36021423339844\n",
      "Cuda Time: 0.7247 ms\n",
      "Inference Latency: 0.0072 ms per inference\n",
      "Memory Footprint: 4707.06 MB\n"
     ]
    }
   ],
   "source": [
    "desc = \"Before Quantization:\"\n",
    "testing(model, test_encodings, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:40:50.412201Z",
     "iopub.status.busy": "2024-11-08T08:40:50.411893Z",
     "iopub.status.idle": "2024-11-08T08:41:02.588751Z",
     "shell.execute_reply": "2024-11-08T08:41:02.587736Z",
     "shell.execute_reply.started": "2024-11-08T08:40:50.412160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/model_before_quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:41:02.593298Z",
     "iopub.status.busy": "2024-11-08T08:41:02.592345Z",
     "iopub.status.idle": "2024-11-08T08:41:04.456896Z",
     "shell.execute_reply": "2024-11-08T08:41:04.456060Z",
     "shell.execute_reply.started": "2024-11-08T08:41:02.593250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:41:04.459404Z",
     "iopub.status.busy": "2024-11-08T08:41:04.459050Z",
     "iopub.status.idle": "2024-11-08T08:41:32.524570Z",
     "shell.execute_reply": "2024-11-08T08:41:32.523719Z",
     "shell.execute_reply.started": "2024-11-08T08:41:04.459351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "replace_linear_layer(model, INT8Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:41:32.525974Z",
     "iopub.status.busy": "2024-11-08T08:41:32.525664Z",
     "iopub.status.idle": "2024-11-08T08:42:57.408370Z",
     "shell.execute_reply": "2024-11-08T08:42:57.407454Z",
     "shell.execute_reply.started": "2024-11-08T08:41:32.525934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Full Quantization:\n",
      "Perplexity: 224.89942932128906\n",
      "Cuda Time: 0.5180 ms\n",
      "Inference Latency: 0.0052 ms per inference\n",
      "Memory Footprint: 1590.76 MB\n"
     ]
    }
   ],
   "source": [
    "desc = \"After Full Quantization:\"\n",
    "testing(model, test_encodings, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:42:57.409938Z",
     "iopub.status.busy": "2024-11-08T08:42:57.409630Z",
     "iopub.status.idle": "2024-11-08T08:43:00.905351Z",
     "shell.execute_reply": "2024-11-08T08:43:00.904474Z",
     "shell.execute_reply.started": "2024-11-08T08:42:57.409903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/model_after_full_quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:43:00.906959Z",
     "iopub.status.busy": "2024-11-08T08:43:00.906638Z",
     "iopub.status.idle": "2024-11-08T08:43:02.506599Z",
     "shell.execute_reply": "2024-11-08T08:43:02.505800Z",
     "shell.execute_reply.started": "2024-11-08T08:43:00.906925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:43:02.508226Z",
     "iopub.status.busy": "2024-11-08T08:43:02.507826Z",
     "iopub.status.idle": "2024-11-08T08:43:12.917590Z",
     "shell.execute_reply": "2024-11-08T08:43:12.916800Z",
     "shell.execute_reply.started": "2024-11-08T08:43:02.508181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "replace_linear_layer(model, INT8Layer, partial=True, quantise_n=partial_no_layers_quantise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:43:12.919144Z",
     "iopub.status.busy": "2024-11-08T08:43:12.918762Z",
     "iopub.status.idle": "2024-11-08T08:44:36.253334Z",
     "shell.execute_reply": "2024-11-08T08:44:36.252380Z",
     "shell.execute_reply.started": "2024-11-08T08:43:12.919101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Partial Quantization on first 12 Layers:\n",
      "Perplexity: 220.3441619873047\n",
      "Cuda Time: 0.5552 ms\n",
      "Inference Latency: 0.0056 ms per inference\n",
      "Memory Footprint: 2292.42 MB\n"
     ]
    }
   ],
   "source": [
    "desc = f\"After Partial Quantization on first {partial_no_layers_quantise} Layers:\"\n",
    "testing(model, test_encodings, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T08:44:36.254731Z",
     "iopub.status.busy": "2024-11-08T08:44:36.254379Z",
     "iopub.status.idle": "2024-11-08T08:44:41.640695Z",
     "shell.execute_reply": "2024-11-08T08:44:41.639759Z",
     "shell.execute_reply.started": "2024-11-08T08:44:36.254697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/model_after_partial_quantization\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2355852,
     "sourceId": 3969419,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
