{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3969419,"sourceType":"datasetVersion","datasetId":2355852}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --upgrade transformers==4.46.2 bitsandbytes>0.37.0","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:24:42.826697Z","iopub.execute_input":"2024-11-08T11:24:42.827471Z","iopub.status.idle":"2024-11-08T11:25:15.283893Z","shell.execute_reply.started":"2024-11-08T11:24:42.827434Z","shell.execute_reply":"2024-11-08T11:25:15.282822Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import profiler\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:15.285890Z","iopub.execute_input":"2024-11-08T11:25:15.286238Z","iopub.status.idle":"2024-11-08T11:25:21.429891Z","shell.execute_reply.started":"2024-11-08T11:25:15.286201Z","shell.execute_reply":"2024-11-08T11:25:21.428900Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n# notebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:17:08.687884Z","iopub.execute_input":"2024-11-08T11:17:08.688488Z","iopub.status.idle":"2024-11-08T11:17:08.782653Z","shell.execute_reply.started":"2024-11-08T11:17:08.688450Z","shell.execute_reply":"2024-11-08T11:17:08.781770Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"test_dataset_path = \"/kaggle/input/ptbdataset/ptb.test.txt\"","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.431068Z","iopub.execute_input":"2024-11-08T11:25:21.431526Z","iopub.status.idle":"2024-11-08T11:25:21.435619Z","shell.execute_reply.started":"2024-11-08T11:25:21.431485Z","shell.execute_reply":"2024-11-08T11:25:21.434745Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model_name = \"allenai/OLMo-1B-hf\"","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.438084Z","iopub.execute_input":"2024-11-08T11:25:21.438834Z","iopub.status.idle":"2024-11-08T11:25:21.451664Z","shell.execute_reply.started":"2024-11-08T11:25:21.438781Z","shell.execute_reply":"2024-11-08T11:25:21.450829Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"bit4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype = torch.bfloat16\n)\n\nbit8_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype = torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.452938Z","iopub.execute_input":"2024-11-08T11:25:21.453396Z","iopub.status.idle":"2024-11-08T11:25:21.463773Z","shell.execute_reply.started":"2024-11-08T11:25:21.453354Z","shell.execute_reply":"2024-11-08T11:25:21.462840Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.465047Z","iopub.execute_input":"2024-11-08T11:25:21.465688Z","iopub.status.idle":"2024-11-08T11:25:21.506176Z","shell.execute_reply.started":"2024-11-08T11:25:21.465644Z","shell.execute_reply":"2024-11-08T11:25:21.505394Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"os.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:25:21.507655Z","iopub.execute_input":"2024-11-08T11:25:21.508053Z","iopub.status.idle":"2024-11-08T11:25:21.517040Z","shell.execute_reply.started":"2024-11-08T11:25:21.508009Z","shell.execute_reply":"2024-11-08T11:25:21.516354Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def read_data(filepath, limit=None):\n    with open(filepath, \"r\") as f:\n        data = f.readlines()\n    data = [line.strip().replace(\"\\n\", \"<eos>\") for line in data]\n    if limit:\n        data = data[:limit]\n    data = \"\\n\".join(data)\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.518047Z","iopub.execute_input":"2024-11-08T11:25:21.518344Z","iopub.status.idle":"2024-11-08T11:25:21.529187Z","shell.execute_reply.started":"2024-11-08T11:25:21.518313Z","shell.execute_reply":"2024-11-08T11:25:21.528367Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def calculate_perplexity(model, encodings):\n    max_length = model.config.max_position_embeddings\n    stride = max_length // 2  # To avoid too much truncation\n    nlls = []\n\n    encodings = encodings.to(device)\n    \n    with profiler.profile(use_device=str(device), use_cpu=False, use_kineto=True) as prof:\n        for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n            begin_loc = i\n            end_loc = min(i + max_length, encodings.input_ids.size(1))\n            trg_len = end_loc - begin_loc  # Target length\n            input_ids = encodings.input_ids[:, begin_loc:end_loc]\n            \n            with torch.no_grad():\n                outputs = model(input_ids, labels=input_ids)\n                neg_log_likelihood = outputs.loss * trg_len\n            \n            nlls.append(neg_log_likelihood)\n\n    perplexity = torch.exp(torch.stack(nlls).sum() / end_loc)\n    profiler_obj = prof.total_average()\n    \n    return perplexity.item(), profiler_obj, len(range(0, encodings.input_ids.size(1), stride))","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.530154Z","iopub.execute_input":"2024-11-08T11:25:21.530449Z","iopub.status.idle":"2024-11-08T11:25:21.539252Z","shell.execute_reply.started":"2024-11-08T11:25:21.530409Z","shell.execute_reply":"2024-11-08T11:25:21.538446Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def testing(model, test_encodings, desc=\"\"):\n    # Run the perplexity calculation and profiling\n    perplexity, profiler_obj, n_items = calculate_perplexity(model, test_encodings)\n    memory_footprint_before_quantization = model.get_memory_footprint() / 1e6\n\n    cuda_time_ms = profiler_obj.device_time / 1e3\n    inference_latency_ms = cuda_time_ms / n_items\n\n    print(desc)\n    print(f\"Perplexity: {perplexity}\")\n    print(f\"Cuda Time: {cuda_time_ms:.4f} ms\")\n    print(f\"Inference Latency: {inference_latency_ms:.4f} ms per inference\")\n    print(f\"Memory Footprint: {memory_footprint_before_quantization:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.542994Z","iopub.execute_input":"2024-11-08T11:25:21.543573Z","iopub.status.idle":"2024-11-08T11:25:21.554982Z","shell.execute_reply.started":"2024-11-08T11:25:21.543520Z","shell.execute_reply":"2024-11-08T11:25:21.554246Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:25:21.556244Z","iopub.execute_input":"2024-11-08T11:25:21.556863Z","iopub.status.idle":"2024-11-08T11:27:37.457698Z","shell.execute_reply.started":"2024-11-08T11:25:21.556820Z","shell.execute_reply":"2024-11-08T11:27:37.456688Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/5.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cc282bdac744349889ccdf1678cb7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f3ff0464e54b96aa951d9d2c254da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"344043e6a83a456bb009cf919529e431"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d61418f6b62407fb5a0e63923aaa202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5683d779db4a415195a25d9207083efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b2eb9855b9408a83e870da5b54cc66"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"test_data = read_data(test_dataset_path)\ntest_encodings = tokenizer(test_data, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:27:37.459215Z","iopub.execute_input":"2024-11-08T11:27:37.459816Z","iopub.status.idle":"2024-11-08T11:27:37.984597Z","shell.execute_reply.started":"2024-11-08T11:27:37.459780Z","shell.execute_reply":"2024-11-08T11:27:37.983583Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"desc = \"Before Quantization:\"\ntesting(model, test_encodings, desc)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:27:37.985702Z","iopub.execute_input":"2024-11-08T11:27:37.986013Z","iopub.status.idle":"2024-11-08T11:28:56.973921Z","shell.execute_reply.started":"2024-11-08T11:27:37.985981Z","shell.execute_reply":"2024-11-08T11:28:56.972910Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [01:09<00:00,  1.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Before Quantization:\nPerplexity: 218.36021423339844\nCuda Time: 0.7250 ms\nInference Latency: 0.0072 ms per inference\nMemory Footprint: 4707.06 MB\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"del model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config=bit8_config, \n    torch_dtype=torch.bfloat16, \n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:28:56.975393Z","iopub.execute_input":"2024-11-08T11:28:56.975817Z","iopub.status.idle":"2024-11-08T11:29:01.622208Z","shell.execute_reply.started":"2024-11-08T11:28:56.975770Z","shell.execute_reply":"2024-11-08T11:29:01.621402Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"desc = \"After 8-bit Quantization:\"\ntesting(model, test_encodings, desc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:29:01.623600Z","iopub.execute_input":"2024-11-08T11:29:01.623911Z","iopub.status.idle":"2024-11-08T11:33:03.099528Z","shell.execute_reply.started":"2024-11-08T11:29:01.623878Z","shell.execute_reply":"2024-11-08T11:33:03.098498Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/100 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n100%|██████████| 100/100 [02:02<00:00,  1.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"After 8-bit Quantization:\nPerplexity: 224.71824645996094\nCuda Time: 0.6221 ms\nInference Latency: 0.0062 ms per inference\nMemory Footprint: 1279.79 MB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.save_pretrained(\"models/model_after_8-bit_quantization\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T09:04:44.704411Z","iopub.execute_input":"2024-11-08T09:04:44.704706Z","iopub.status.idle":"2024-11-08T09:04:46.976076Z","shell.execute_reply.started":"2024-11-08T09:04:44.704675Z","shell.execute_reply":"2024-11-08T09:04:46.974989Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"del model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bit4_config,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:33:03.100982Z","iopub.execute_input":"2024-11-08T11:33:03.101665Z","iopub.status.idle":"2024-11-08T11:33:05.293722Z","shell.execute_reply.started":"2024-11-08T11:33:03.101618Z","shell.execute_reply":"2024-11-08T11:33:05.292745Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"desc = \"After 4-bit Quantization:\"\ntesting(model, test_encodings, desc)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:33:05.294971Z","iopub.execute_input":"2024-11-08T11:33:05.295307Z","iopub.status.idle":"2024-11-08T11:35:01.915147Z","shell.execute_reply.started":"2024-11-08T11:33:05.295273Z","shell.execute_reply":"2024-11-08T11:35:01.914250Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [01:40<00:00,  1.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"After 4-bit Quantization:\nPerplexity: 270.70428466796875\nCuda Time: 0.5517 ms\nInference Latency: 0.0055 ms per inference\nMemory Footprint: 742.92 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model.save_pretrained(\"models/model_after_4-bit_quantization\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T09:06:46.095130Z","iopub.execute_input":"2024-11-08T09:06:46.095993Z","iopub.status.idle":"2024-11-08T09:06:47.095983Z","shell.execute_reply.started":"2024-11-08T09:06:46.095941Z","shell.execute_reply":"2024-11-08T09:06:47.095145Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"del model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=nf4_config,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:35:01.916450Z","iopub.execute_input":"2024-11-08T11:35:01.916768Z","iopub.status.idle":"2024-11-08T11:35:04.071095Z","shell.execute_reply.started":"2024-11-08T11:35:01.916735Z","shell.execute_reply":"2024-11-08T11:35:04.070322Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"desc = \"After NF4-bit Quantization:\"\ntesting(model, test_encodings, desc)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:35:04.072114Z","iopub.execute_input":"2024-11-08T11:35:04.072419Z","iopub.status.idle":"2024-11-08T11:37:01.392400Z","shell.execute_reply.started":"2024-11-08T11:35:04.072388Z","shell.execute_reply":"2024-11-08T11:37:01.391314Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [01:41<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"After NF4-bit Quantization:\nPerplexity: 243.39186096191406\nCuda Time: 0.5557 ms\nInference Latency: 0.0056 ms per inference\nMemory Footprint: 742.92 MB\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"model.save_pretrained(\"models/model_after_nf-4_quantization\") #, push_to_hub=True, repo_id=\"bhavberi/OLMo-1B-NF4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:17:13.355204Z","iopub.execute_input":"2024-11-08T11:17:13.355601Z","iopub.status.idle":"2024-11-08T11:17:41.852606Z","shell.execute_reply.started":"2024-11-08T11:17:13.355566Z","shell.execute_reply":"2024-11-08T11:17:41.851817Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/810M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ca877822624c8b86ac529580ac258b"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    \"bhavberi/OLMo-1B-NF4\",\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:37:01.393528Z","iopub.execute_input":"2024-11-08T11:37:01.393847Z","iopub.status.idle":"2024-11-08T11:37:34.274002Z","shell.execute_reply.started":"2024-11-08T11:37:01.393814Z","shell.execute_reply":"2024-11-08T11:37:34.273234Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4adf703e72e24985b8e5cc27552011e9"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/810M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085f09e0c3a44e18973bd6de1769e2e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2239bb4abf1240528075bb52f25d7f3c"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"desc = \"After NF4-bit Quantization HF:\"\ntesting(model, test_encodings, desc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T11:37:34.275334Z","iopub.execute_input":"2024-11-08T11:37:34.275658Z","iopub.status.idle":"2024-11-08T11:39:31.579752Z","shell.execute_reply.started":"2024-11-08T11:37:34.275623Z","shell.execute_reply":"2024-11-08T11:39:31.578797Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [01:41<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"After NF4-bit Quantization HF:\nPerplexity: 243.39186096191406\nCuda Time: 0.5556 ms\nInference Latency: 0.0056 ms per inference\nMemory Footprint: 742.92 MB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
