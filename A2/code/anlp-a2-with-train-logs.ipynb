{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9525328,"sourceType":"datasetVersion","datasetId":5800233}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchinfo import summary\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.004457Z","iopub.execute_input":"2024-10-02T09:29:11.005342Z","iopub.status.idle":"2024-10-02T09:29:11.011890Z","shell.execute_reply.started":"2024-10-02T09:29:11.005284Z","shell.execute_reply":"2024-10-02T09:29:11.010829Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nnltk.download(\"punkt\")\nnltk.download(\"punkt_tab\")\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.013587Z","iopub.execute_input":"2024-10-02T09:29:11.013901Z","iopub.status.idle":"2024-10-02T09:29:11.036786Z","shell.execute_reply.started":"2024-10-02T09:29:11.013869Z","shell.execute_reply":"2024-10-02T09:29:11.035884Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"en_train = '/kaggle/input/ted-talks-corpus/train.en'\nfr_train = '/kaggle/input/ted-talks-corpus/train.fr'\nen_val = '/kaggle/input/ted-talks-corpus/dev.en'\nfr_val = '/kaggle/input/ted-talks-corpus/dev.fr'\nen_test = '/kaggle/input/ted-talks-corpus/test.en'\nfr_test = '/kaggle/input/ted-talks-corpus/test.fr'","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.037927Z","iopub.execute_input":"2024-10-02T09:29:11.038276Z","iopub.status.idle":"2024-10-02T09:29:11.043027Z","shell.execute_reply.started":"2024-10-02T09:29:11.038242Z","shell.execute_reply":"2024-10-02T09:29:11.042087Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"train = False\npadding_before = False\nplot_losses = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 300\nmax_length = 64\nlr=1e-4\n\nheads = 6\nlayers = 6\n\nepochs = 15\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.054352Z","iopub.execute_input":"2024-10-02T09:29:11.054961Z","iopub.status.idle":"2024-10-02T09:29:11.064867Z","shell.execute_reply.started":"2024-10-02T09:29:11.054916Z","shell.execute_reply":"2024-10-02T09:29:11.063946Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"models\", exist_ok=True)\n\nsave_path=\"./models/transformer\"\n\nsave_path = save_path + f\"_heads{heads}_layers{layers}\"\n\nsave_path = save_path + \".pth\"\n\nprint(f\"Saving model to {save_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.066248Z","iopub.execute_input":"2024-10-02T09:29:11.067120Z","iopub.status.idle":"2024-10-02T09:29:11.076523Z","shell.execute_reply.started":"2024-10-02T09:29:11.067072Z","shell.execute_reply":"2024-10-02T09:29:11.075602Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Saving model to ./models/transformer_heads6_layers6.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"random_seed = 42\n\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\nprint(\"Using Random Seed:\", random_seed)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.077766Z","iopub.execute_input":"2024-10-02T09:29:11.078098Z","iopub.status.idle":"2024-10-02T09:29:11.089549Z","shell.execute_reply.started":"2024-10-02T09:29:11.078035Z","shell.execute_reply":"2024-10-02T09:29:11.088602Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Using Random Seed: 42\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.090649Z","iopub.execute_input":"2024-10-02T09:29:11.091086Z","iopub.status.idle":"2024-10-02T09:29:11.096644Z","shell.execute_reply.started":"2024-10-02T09:29:11.091025Z","shell.execute_reply":"2024-10-02T09:29:11.095699Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = str(text).lower().strip()\n    text = text.rstrip('\\n')\n#     text = re.sub(r\"<[^>]+>\", \"\", text)\n    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s.,;!?':()\\[\\]{}-]\", \" \", text)  # Keep selected punctuation marks, symbols and apostrophes\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")  # Corrected encoding\n\n    return text\n\ndef clean_sentences(sentences):\n    sentences = [clean_text(sentence) for sentence in sentences]\n    sentences = [s for s in sentences if s and s != \"\"]  # remove empty strings\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.098814Z","iopub.execute_input":"2024-10-02T09:29:11.099152Z","iopub.status.idle":"2024-10-02T09:29:11.108718Z","shell.execute_reply.started":"2024-10-02T09:29:11.099106Z","shell.execute_reply":"2024-10-02T09:29:11.107894Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def read_data(en_path, fr_path):\n    with open(en_path, \"r\") as f:\n        en_data = f.readlines()\n    with open(fr_path, \"r\") as f:\n        fr_data = f.readlines()\n\n    assert len(en_data) == len(fr_data), \"Data mismatch\"\n\n    en_data = clean_sentences(en_data)\n    fr_data = clean_sentences(fr_data)\n\n    assert len(en_data) == len(fr_data), \"Data mismatch in cleaned data\"\n\n    return en_data, fr_data\n\ndef word_tokenizer(sentence):\n    words = word_tokenize(sentence)\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.122663Z","iopub.execute_input":"2024-10-02T09:29:11.123356Z","iopub.status.idle":"2024-10-02T09:29:11.130224Z","shell.execute_reply.started":"2024-10-02T09:29:11.123315Z","shell.execute_reply":"2024-10-02T09:29:11.129296Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def flatten_concatenation(list_of_lists, unique=False):\n    # flat_list = []\n    # for sublist in list_of_lists:\n    #     flat_list += sublist\n\n    # flat_list = list(set(flat_list))\n    # return flat_list\n    flat_array = np.concatenate(list_of_lists)\n    if unique:\n        flat_list = np.unique(flat_array).tolist()\n    else:\n        flat_list = flat_array.tolist()\n    return flat_list","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.132350Z","iopub.execute_input":"2024-10-02T09:29:11.132777Z","iopub.status.idle":"2024-10-02T09:29:11.140921Z","shell.execute_reply.started":"2024-10-02T09:29:11.132743Z","shell.execute_reply":"2024-10-02T09:29:11.139836Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def reverse_vocab(vocab):\n    return {v: k for k, v in vocab.items()}","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.142371Z","iopub.execute_input":"2024-10-02T09:29:11.142745Z","iopub.status.idle":"2024-10-02T09:29:11.150727Z","shell.execute_reply.started":"2024-10-02T09:29:11.142704Z","shell.execute_reply":"2024-10-02T09:29:11.149934Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def return_words_till_EOS(lst, eos=2):\n    if eos not in lst:\n        return lst\n    return lst[:lst.index(eos)]","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.151824Z","iopub.execute_input":"2024-10-02T09:29:11.152168Z","iopub.status.idle":"2024-10-02T09:29:11.160550Z","shell.execute_reply.started":"2024-10-02T09:29:11.152135Z","shell.execute_reply":"2024-10-02T09:29:11.159671Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"def pad_sequence(sequence, max_len, before=True, pad_token=0):\n    if len(sequence) > max_len:\n        return sequence[:max_len]\n    elif before:\n        return [pad_token] * (max_len - len(sequence)) + sequence\n    else:\n        return sequence + [pad_token] * (max_len - len(sequence))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.163767Z","iopub.execute_input":"2024-10-02T09:29:11.164065Z","iopub.status.idle":"2024-10-02T09:29:11.170686Z","shell.execute_reply.started":"2024-10-02T09:29:11.164016Z","shell.execute_reply":"2024-10-02T09:29:11.169785Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(\n        self,\n        en_data,\n        fr_data,\n        en_vocab,\n        fr_vocab,\n        pad_before=False,\n    ):\n        self.en_data = []\n        self.fr_data = []\n        self.labels = []\n        self.en_vocab = en_vocab\n        self.fr_vocab = fr_vocab\n\n        assert len(en_data) == len(fr_data)\n        self.length = len(en_data)\n\n        en_pad = self.en_vocab[\"<pad>\"]\n        en_unk = self.en_vocab[\"<unk>\"]\n        en_sos = self.en_vocab[\"<sos>\"]\n        en_eos = self.en_vocab[\"<eos>\"]\n        fr_pad = self.fr_vocab[\"<pad>\"]\n        fr_unk = self.fr_vocab[\"<unk>\"]\n        fr_sos = self.fr_vocab[\"<sos>\"]\n        fr_eos = self.fr_vocab[\"<eos>\"]\n\n        tqdm_obj = tqdm(\n            total=self.length, desc=\"Creating dataset\"\n        )\n        for index, (en_sentence, fr_sentence) in enumerate(zip(en_data, fr_data)):\n            en_indices = [int(self.en_vocab.get(w, en_unk)) for w in en_sentence]\n            en_indices = [en_sos] + en_indices[: max_length - 2] + [en_eos]\n            en_indices = pad_sequence(\n                en_indices, max_length, before=pad_before, pad_token=en_pad\n            )\n            self.en_data.append(\n                torch.tensor(en_indices, dtype=torch.int, device=device)\n            )\n\n            fr_indices1 = [int(self.fr_vocab.get(w, fr_unk)) for w in fr_sentence]\n            fr_indices = [fr_sos] + fr_indices1\n            fr_indices = pad_sequence(\n                fr_indices, max_length, before=pad_before, pad_token=fr_pad\n            )\n            self.fr_data.append(\n                torch.tensor(fr_indices, dtype=torch.int, device=device)\n            )\n\n            fr_indices = fr_indices1 + [fr_eos]\n            fr_indices = pad_sequence(\n                fr_indices, max_length, before=pad_before, pad_token=fr_pad\n            )\n            self.labels.append(torch.tensor(fr_indices, device=device))\n\n            if index % 10 == 0:\n                tqdm_obj.update(10)\n\n        tqdm_obj.close()\n\n        print(f\"Dataset created with {self.length} samples\")\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        return self.en_data[idx], self.fr_data[idx], self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.172267Z","iopub.execute_input":"2024-10-02T09:29:11.172966Z","iopub.status.idle":"2024-10-02T09:29:11.188038Z","shell.execute_reply.started":"2024-10-02T09:29:11.172920Z","shell.execute_reply":"2024-10-02T09:29:11.187116Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"def create_positional_encoding(max_length, embedding_dim):\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim)\n    )\n    pe[:, 0::2] = torch.sin(position.float() * div_term)\n    pe[:, 1::2] = torch.cos(position.float() * div_term)\n    return pe.to(device)\n\n\ndef make_src_mask(src):\n    src1 = src\n    if len(src.shape) == 3:\n        src1 = torch.sum(src, dim=-1)\n\n    src_mask = (src1 != 0).unsqueeze(1).unsqueeze(2)\n    return src_mask.to(device)\n\n\ndef make_trg_mask(trg):\n    trg1 = trg\n    if len(trg.shape) == 3:\n        trg1 = torch.sum(trg, dim=-1)\n    \n    n, trg_len = trg1.size()\n    trg_mask = torch.tril(torch.ones(trg_len, trg_len)).expand(n, 1, trg_len, trg_len)\n    return trg_mask.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.189339Z","iopub.execute_input":"2024-10-02T09:29:11.189673Z","iopub.status.idle":"2024-10-02T09:29:11.202777Z","shell.execute_reply.started":"2024-10-02T09:29:11.189626Z","shell.execute_reply":"2024-10-02T09:29:11.201902Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embedding_dim: int = 512, num_heads: int = 8):\n        super(MultiHeadAttention, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.head_dim = embedding_dim // num_heads\n\n        assert (\n            self.head_dim * num_heads == embedding_dim\n        ), \"Embedding dimension must be divisible by number of heads\"\n\n        self.q = nn.Linear(self.head_dim, self.head_dim)\n        self.k = nn.Linear(self.head_dim, self.head_dim)\n        self.v = nn.Linear(self.head_dim, self.head_dim)\n        self.fc = nn.Linear(self.embedding_dim, self.embedding_dim)\n\n    def forward(self, value, key, query, mask):\n        n = query.size(0)\n        query_len, key_len, value_len = query.size(1), key.size(1), value.size(1)\n\n        value = self.v(value.reshape(n, value_len, self.num_heads, self.head_dim))\n        query = self.q(query.reshape(n, query_len, self.num_heads, self.head_dim))\n        key = self.k(key.reshape(n, key_len, self.num_heads, self.head_dim))\n\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [query, key])\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -float(\"inf\"))\n        attention = F.softmax(energy / np.sqrt(self.head_dim), dim=3)\n\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, value]).reshape(\n            n, query_len, self.embedding_dim\n        )\n        out = self.fc(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.204029Z","iopub.execute_input":"2024-10-02T09:29:11.204442Z","iopub.status.idle":"2024-10-02T09:29:11.216459Z","shell.execute_reply.started":"2024-10-02T09:29:11.204397Z","shell.execute_reply":"2024-10-02T09:29:11.215536Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        embed_size: int,\n        heads: int,\n        forward_expansion: int,\n        dropout: float,\n    ):\n        super(TransformerBlock, self).__init__()\n        self.attention = MultiHeadAttention(embed_size, heads)\n\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_size, forward_expansion * embed_size),\n            nn.ReLU(),\n            nn.Linear(forward_expansion * embed_size, embed_size),\n        )\n\n        self.layer_norm1 = nn.Sequential(\n            nn.LayerNorm(embed_size),\n            nn.Dropout(dropout),\n        )\n        self.layer_norm2 = nn.Sequential(\n            nn.LayerNorm(embed_size),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, value, key, query, mask):\n        attention = self.attention(value, key, query, mask)\n        x = self.layer_norm1(attention + query)\n        forward = self.feed_forward(x)\n        out = self.layer_norm2(forward + x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.217852Z","iopub.execute_input":"2024-10-02T09:29:11.218250Z","iopub.status.idle":"2024-10-02T09:29:11.231346Z","shell.execute_reply.started":"2024-10-02T09:29:11.218205Z","shell.execute_reply":"2024-10-02T09:29:11.230435Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(\n        self,\n        src_vocab_size: int,\n        embed_size: int,\n        num_layers: int,\n        heads: int,\n        forward_expansion: int,\n        dropout: float,\n        max_len: int,\n    ):\n        super(Encoder, self).__init__()\n        self.embed_size = embed_size\n        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n        self.position_embedding = nn.Embedding(max_len, embed_size)\n        self.layers = nn.ModuleList(\n            [\n                TransformerBlock(embed_size, heads, forward_expansion, dropout)\n                for _ in range(num_layers)\n            ]\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        n, seq_len = x.size()\n        positions = torch.arange(0, seq_len).expand(n, seq_len).to(device)\n        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n        for layer in self.layers:\n            out = layer(out, out, out, mask)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.232433Z","iopub.execute_input":"2024-10-02T09:29:11.232710Z","iopub.status.idle":"2024-10-02T09:29:11.246080Z","shell.execute_reply.started":"2024-10-02T09:29:11.232679Z","shell.execute_reply":"2024-10-02T09:29:11.245123Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(\n        self, embed_size: int, heads: int, forward_expansion: int, dropout: float\n    ):\n        super(DecoderBlock, self).__init__()\n        self.attention = MultiHeadAttention(embed_size, heads)\n        self.transformer_block = TransformerBlock(\n            embed_size, heads, forward_expansion, dropout\n        )\n        self.layer_norm = nn.Sequential(\n            nn.LayerNorm(embed_size),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x, value, key, src_mask, trg_mask):\n        attention = self.attention(x, x, x, trg_mask)\n        query = self.layer_norm(attention + x)\n        out = self.transformer_block(value, key, query, src_mask)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.247368Z","iopub.execute_input":"2024-10-02T09:29:11.247705Z","iopub.status.idle":"2024-10-02T09:29:11.256364Z","shell.execute_reply.started":"2024-10-02T09:29:11.247671Z","shell.execute_reply":"2024-10-02T09:29:11.255416Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(\n        self,\n        trg_vocab_size: int,\n        embed_size: int,\n        num_layers: int,\n        heads: int,\n        forward_expansion: int,\n        dropout: float,\n        max_len: int,\n    ):\n        super(Decoder, self).__init__()\n        self.device = device\n        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n        self.position_embedding = nn.Embedding(max_len, embed_size)\n        self.layers = nn.ModuleList(\n            [\n                DecoderBlock(embed_size, heads, forward_expansion, dropout)\n                for _ in range(num_layers)\n            ]\n        )\n        self.fc = nn.Linear(embed_size, trg_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, src_mask, trg_mask):\n        n, seq_len = x.size()\n        positions = torch.arange(0, seq_len).expand(n, seq_len).to(device)\n        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n        for layer in self.layers:\n            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n        out = self.fc(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.257411Z","iopub.execute_input":"2024-10-02T09:29:11.257700Z","iopub.status.idle":"2024-10-02T09:29:11.271036Z","shell.execute_reply.started":"2024-10-02T09:29:11.257668Z","shell.execute_reply":"2024-10-02T09:29:11.270085Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(\n        self,\n        src_vocab_size: int,\n        trg_vocab_size: int,\n        embed_size: int = 512,\n        num_layers: int = 6,\n        forward_expansion: int = 4,\n        heads: int = 8,\n        dropout: float = 0.2,\n        max_len: int = 50,\n        save_path=None,\n    ):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(\n            src_vocab_size,\n            embed_size,\n            num_layers,\n            heads,\n            forward_expansion,\n            dropout,\n            max_len,\n        )\n        self.decoder = Decoder(\n            trg_vocab_size,\n            embed_size,\n            num_layers,\n            heads,\n            forward_expansion,\n            dropout,\n            max_len,\n        )\n        self.best_val_loss = float(\"inf\")\n        self.save_path = save_path\n\n    def forward(self, src, trg):\n        src_mask = make_src_mask(src)\n        trg_mask = make_trg_mask(trg)\n        enc_src = self.encoder(src, src_mask)\n        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n        return out\n\n    def fit(self, train_loader, val_loader, criterion, optimizer, num_epochs: int = 10):\n        train_losses = []\n        val_losses = []\n\n        for epoch in range(num_epochs):\n            self.train()\n            train_loss = 0\n            for src, trg, label in tqdm(train_loader, total=len(train_loader)):\n                optimizer.zero_grad()\n                output = self(src, trg)\n                output = output.reshape(-1, output.size(-1))\n                label = label.reshape(-1)\n\n                loss = criterion(output, label)\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n            train_loss /= len(train_loader)\n            train_losses.append(train_loss)\n            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss}\")\n\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n            val_loss = self.evaluate(val_loader, criterion, True)\n            val_losses.append(val_loss)\n            print(f\"Validation Loss: {val_loss}\")\n\n            if self.save_path and val_loss < self.best_val_loss:\n                self.best_val_loss = val_loss\n                torch.save(self.state_dict(), self.save_path)\n\n        return train_losses, val_losses\n\n    def evaluate(self, val_loader, criterion, tqdm_disabled: bool = False):\n        self.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for src, trg_input, trg_target in tqdm(\n                val_loader, total=len(val_loader), disable=tqdm_disabled\n            ):\n                output = self(src, trg_input)\n                output_dim = output.shape[-1]\n                output = output.view(-1, output_dim)\n                trg_target = trg_target.view(-1)\n                loss = criterion(output, trg_target)\n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n        return val_loss\n\n    def load(self, path=None):\n        if path:\n            self.load_state_dict(torch.load(path))\n        elif self.save_path:\n            self.load_state_dict(torch.load(self.save_path))\n        else:\n            raise ValueError(\"No model path provided\")\n\n    def predict(\n        self,\n        src,\n        src_preprocessed=False,\n        max_len=64,\n        return_sentence=False,\n        fr_vocab=None,\n        en_vocab=None,\n        start_token_idx=1,\n        end_token_idx=2,\n    ):\n        self.eval()  # Set the model to evaluation mode\n\n        if not src_preprocessed:\n            assert en_vocab is not None\n            src = word_tokenizer(src)\n            src = [en_vocab.get(w, en_vocab[\"<unk>\"]) for w in src]\n            src = [start_token_idx] + src[: max_len - 2] + [end_token_idx]\n            src = pad_sequence(src, max_len, before=padding_before, pad_token=0)\n            src = torch.tensor([src], dtype=torch.int, device=device)\n\n        trg = torch.tensor([[start_token_idx]], dtype=torch.int, device=device)\n\n        src_mask = make_src_mask(src)\n        with torch.no_grad():\n            enc_src = self.encoder(src, src_mask)\n\n        for _ in range(max_len):\n            trg_mask = make_trg_mask(trg)\n\n            with torch.no_grad():\n                output = self.decoder(trg, enc_src, src_mask, trg_mask)\n                output = output[:, -1]\n\n            next_token = output.argmax(-1).unsqueeze(0)\n            trg = torch.cat((trg, next_token), dim=1)\n\n            if next_token.item() == end_token_idx:\n                break\n\n        generated_sequence = trg.squeeze(0).tolist()[1:]\n        if generated_sequence[-1] == 2:\n            generated_sequence = generated_sequence[:-1]\n\n        if return_sentence:\n            assert fr_vocab is not None\n            fr_vocab_rev = reverse_vocab(fr_vocab)\n            generated_sequence = [fr_vocab_rev[idx] for idx in generated_sequence]\n\n        return generated_sequence\n\n    def test(self, test_loader, en_vocab, fr_vocab):\n        self.eval()\n        bleu_scores = []\n\n        reverse_fr_vocab = reverse_vocab(fr_vocab)\n\n        with torch.no_grad():\n            for src, _, label in tqdm(test_loader, total=len(test_loader)):\n                for i in range(src.size(0)):\n                    src_i = src[i].unsqueeze(0)\n                    trg_target_i = label[i].unsqueeze(0)\n\n                    candidate = self.predict(\n                        src_i,\n                        src_preprocessed=True,\n                        max_len=max_length,\n                        fr_vocab=fr_vocab,\n                        start_token_idx=en_vocab[\"<sos>\"],\n                        end_token_idx=en_vocab[\"<eos>\"],\n                    )\n                    candidate = [reverse_fr_vocab[idx] for idx in candidate]\n\n                    reference = return_words_till_EOS(\n                        trg_target_i.squeeze(0).tolist(), eos=fr_vocab[\"<eos>\"]\n                    )\n                    reference = [reverse_fr_vocab[idx] for idx in reference]\n\n                    bleu_score = sentence_bleu(\n                        [reference],\n                        candidate,\n                        smoothing_function=SmoothingFunction().method1,\n                    )\n                    bleu_scores.append(bleu_score)\n\n        print(f\"Test BLEU Score: {np.mean(bleu_scores)}\")\n        # print(\"BLEU Scores:\", bleu_scores)\n\n        if device == \"cuda\":\n            torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.272674Z","iopub.execute_input":"2024-10-02T09:29:11.273014Z","iopub.status.idle":"2024-10-02T09:29:11.305880Z","shell.execute_reply.started":"2024-10-02T09:29:11.272981Z","shell.execute_reply":"2024-10-02T09:29:11.304871Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"train_en, train_fr = read_data(en_train, fr_train)\nval_en, val_fr = read_data(en_val, fr_val)\ntest_en, test_fr = read_data(en_test, fr_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:11.309874Z","iopub.execute_input":"2024-10-02T09:29:11.310674Z","iopub.status.idle":"2024-10-02T09:29:12.370631Z","shell.execute_reply.started":"2024-10-02T09:29:11.310624Z","shell.execute_reply":"2024-10-02T09:29:12.369763Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"train_en_words = [word_tokenizer(s) for s in train_en]\ntrain_fr_words = [word_tokenizer(s) for s in train_fr]\nval_en_words = [word_tokenizer(s) for s in val_en]\nval_fr_words = [word_tokenizer(s) for s in val_fr]\ntest_en_words = [word_tokenizer(s) for s in test_en]\ntest_fr_words = [word_tokenizer(s) for s in test_fr]\n\nall_en_words = flatten_concatenation(train_en_words + val_en_words + test_en_words)\nall_fr_words = flatten_concatenation(train_fr_words + val_fr_words + test_fr_words)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:12.371834Z","iopub.execute_input":"2024-10-02T09:29:12.372240Z","iopub.status.idle":"2024-10-02T09:29:28.652610Z","shell.execute_reply.started":"2024-10-02T09:29:12.372196Z","shell.execute_reply":"2024-10-02T09:29:28.651689Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"en_word_counts = Counter(all_en_words)\nassert en_word_counts.total() == len(all_en_words)\nfr_word_counts = Counter(all_fr_words)\nassert fr_word_counts.total() == len(all_fr_words)\n\nen_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\nfr_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n\nfor word, count in en_word_counts.items():\n    # if count > 1:\n        en_vocab[word] = len(en_vocab.keys())\n\nfor word, count in fr_word_counts.items():\n    # if count > 1:\n        fr_vocab[word] = len(fr_vocab.keys())","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:28.653917Z","iopub.execute_input":"2024-10-02T09:29:28.654787Z","iopub.status.idle":"2024-10-02T09:29:28.847686Z","shell.execute_reply.started":"2024-10-02T09:29:28.654739Z","shell.execute_reply":"2024-10-02T09:29:28.846695Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"if train:\n    train_dataset = MyDataset(\n        train_en_words,\n        train_fr_words,\n        en_vocab,\n        fr_vocab,\n        padding_before,\n    )\n    val_dataset = MyDataset(\n        val_en_words,\n        val_fr_words,\n        en_vocab,\n        fr_vocab,\n        padding_before,\n    )\ntest_dataset = MyDataset(\n    test_en_words,\n    test_fr_words,\n    en_vocab,\n    fr_vocab,\n    padding_before,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:28.849261Z","iopub.execute_input":"2024-10-02T09:29:28.849587Z","iopub.status.idle":"2024-10-02T09:29:33.371022Z","shell.execute_reply.started":"2024-10-02T09:29:28.849552Z","shell.execute_reply":"2024-10-02T09:29:33.370178Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"Creating dataset: 100%|██████████| 30000/30000 [00:04<00:00, 7149.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset created with 30000 samples\n","output_type":"stream"},{"name":"stderr","text":"Creating dataset: 890it [00:00, 7017.12it/s]                         \n","output_type":"stream"},{"name":"stdout","text":"Dataset created with 887 samples\n","output_type":"stream"},{"name":"stderr","text":"Creating dataset: 1310it [00:00, 7131.28it/s]                         ","output_type":"stream"},{"name":"stdout","text":"Dataset created with 1305 samples\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"if train:\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n    )\n    print(\"Length of train_loader:\", len(train_loader))\n    print(\"Length of val_loader:\", len(val_loader))\n\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\nprint(\"Length of test_loader:\", len(test_loader))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:33.372215Z","iopub.execute_input":"2024-10-02T09:29:33.372537Z","iopub.status.idle":"2024-10-02T09:29:33.631541Z","shell.execute_reply.started":"2024-10-02T09:29:33.372505Z","shell.execute_reply":"2024-10-02T09:29:33.630529Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Length of train_loader: 938\nLength of val_loader: 28\nLength of test_loader: 1305\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Transformer(\n    len(en_vocab),\n    len(fr_vocab),\n    embed_size=embedding_dim,\n    num_layers=layers,\n    heads=heads,\n    forward_expansion=4,\n    dropout=0.2,\n    max_len=max_length,\n    save_path=save_path,\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:33.635013Z","iopub.execute_input":"2024-10-02T09:29:33.635366Z","iopub.status.idle":"2024-10-02T09:29:34.342810Z","shell.execute_reply.started":"2024-10-02T09:29:33.635333Z","shell.execute_reply":"2024-10-02T09:29:34.341894Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# print(model)\n# summary(model, input_data=[\n#     torch.randint(low=0, high=len(en_vocab), size=(batch_size, max_length), device=device)\n#     torch.randint(low=0, high=len(fr_vocab), size=(batch_size, max_length), device=device)\n# ])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:34.343925Z","iopub.execute_input":"2024-10-02T09:29:34.344243Z","iopub.status.idle":"2024-10-02T09:29:34.348201Z","shell.execute_reply.started":"2024-10-02T09:29:34.344209Z","shell.execute_reply":"2024-10-02T09:29:34.347267Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=lr) # type: ignore\ncriterion = nn.CrossEntropyLoss(ignore_index=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:34.349403Z","iopub.execute_input":"2024-10-02T09:29:34.349704Z","iopub.status.idle":"2024-10-02T09:29:34.362306Z","shell.execute_reply.started":"2024-10-02T09:29:34.349670Z","shell.execute_reply":"2024-10-02T09:29:34.361506Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"if train:\n    train_losses, val_losses = model.fit(\n        train_loader,\n        val_loader,\n        criterion,\n        optimizer,\n        num_epochs=epochs,\n    )\n\n    if plot_losses:\n        plt.plot(train_losses, label=\"Train Loss\")\n        plt.plot(val_losses, label=\"Validation Loss\")\n        plt.legend()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:29:34.363325Z","iopub.execute_input":"2024-10-02T09:29:34.363642Z","iopub.status.idle":"2024-10-02T09:50:08.678891Z","shell.execute_reply.started":"2024-10-02T09:29:34.363611Z","shell.execute_reply":"2024-10-02T09:50:08.677833Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Loss: 6.90152710282218\nValidation Loss: 6.569449509893145\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15, Loss: 6.272208140094651\nValidation Loss: 6.2743649653026035\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15, Loss: 5.933016585388672\nValidation Loss: 6.088992510523115\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15, Loss: 5.707155293238951\nValidation Loss: 5.966449567249843\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15, Loss: 5.5347091053594655\nValidation Loss: 5.874079857553754\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:20<00:00, 11.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15, Loss: 5.390632296421889\nValidation Loss: 5.781954424721854\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15, Loss: 5.270443738904843\nValidation Loss: 5.727157711982727\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15, Loss: 5.170359751308904\nValidation Loss: 5.649943845612662\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15, Loss: 5.0770362841803385\nValidation Loss: 5.600454092025757\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15, Loss: 4.992152666994758\nValidation Loss: 5.543810163225446\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15, Loss: 4.917658916160242\nValidation Loss: 5.544030870710101\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15, Loss: 4.850807659661592\nValidation Loss: 5.512316295078823\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15, Loss: 4.785940251624915\nValidation Loss: 5.469122852597918\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15, Loss: 4.726124031457311\nValidation Loss: 5.45763145174299\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [01:21<00:00, 11.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15, Loss: 4.675078034146762\nValidation Loss: 5.429043531417847\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:50:08.681086Z","iopub.execute_input":"2024-10-02T09:50:08.681403Z","iopub.status.idle":"2024-10-02T09:50:08.837547Z","shell.execute_reply.started":"2024-10-02T09:50:08.681370Z","shell.execute_reply":"2024-10-02T09:50:08.836603Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/593088400.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.load_state_dict(torch.load(self.save_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(test_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:50:08.838999Z","iopub.execute_input":"2024-10-02T09:50:08.839400Z","iopub.status.idle":"2024-10-02T09:50:27.165971Z","shell.execute_reply.started":"2024-10-02T09:50:08.839357Z","shell.execute_reply":"2024-10-02T09:50:27.164959Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"100%|██████████| 1305/1305 [00:18<00:00, 71.26it/s]\n","output_type":"stream"},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"4.895798279407837"},"metadata":{}}]},{"cell_type":"code","source":"model.predict(\n    \"I am a student.\",\n    en_vocab=en_vocab,\n    fr_vocab=fr_vocab,\n    return_sentence=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:50:27.167236Z","iopub.execute_input":"2024-10-02T09:50:27.167680Z","iopub.status.idle":"2024-10-02T09:50:27.328304Z","shell.execute_reply.started":"2024-10-02T09:50:27.167634Z","shell.execute_reply":"2024-10-02T09:50:27.319517Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"['les', 'gens', 'sont', 'un', 'peu', 'de', 'la', 'vie', '.']"},"metadata":{}}]},{"cell_type":"code","source":"model.test(test_loader, en_vocab, fr_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:50:27.329897Z","iopub.execute_input":"2024-10-02T09:50:27.335965Z","iopub.status.idle":"2024-10-02T09:56:09.491369Z","shell.execute_reply.started":"2024-10-02T09:50:27.335916Z","shell.execute_reply":"2024-10-02T09:56:09.490392Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"100%|██████████| 1305/1305 [05:42<00:00,  3.81it/s]","output_type":"stream"},{"name":"stdout","text":"Test BLEU Score: 0.031163257459442834\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}